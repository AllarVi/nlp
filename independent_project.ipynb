{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "independent_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AllarVi/nlp/blob/master/independent_project.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "jf1yKTW1Dsn9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upload dataset"
      ]
    },
    {
      "metadata": {
        "id": "u5F8sDHPrv8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "0977436e-28b7-4866-e1ab-c5b163d502c3"
      },
      "cell_type": "code",
      "source": [
        "! rm -f que-tag.txt\n",
        "! wget https://raw.githubusercontent.com/AllarVi/nlp/master/tag-que.txt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-05-16 08:55:20--  https://raw.githubusercontent.com/AllarVi/nlp/master/tag-que.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99233319 (95M) [text/plain]\n",
            "Saving to: ‘tag-que.txt’\n",
            "\n",
            "tag-que.txt         100%[===================>]  94.64M   135MB/s    in 0.7s    \n",
            "\n",
            "2018-05-16 08:55:22 (135 MB/s) - ‘tag-que.txt’ saved [99233319/99233319]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3kqxgU1jCaWL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea551f5c-3ed0-4fdf-cd38-013f682abc1c"
      },
      "cell_type": "code",
      "source": [
        "for l in open('tag-que.txt'):\n",
        "  print(l)\n",
        "  break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flex actionscript-3 air\tSQLStatement.execute() - multiple queries in one statement\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TxybzSj8DwsG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "793GQiBWDbTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f1905af8-f9bf-4b91-a20c-b80ef08857e5"
      },
      "cell_type": "code",
      "source": [
        "! pip install torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 26kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5b2c8000 @  0x7ff555a651c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XT05wCslDb8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6628d28-6329-43c0-89ec-2e7cd60d4a9d"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W1nmIRjfM2oS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ]
    },
    {
      "metadata": {
        "id": "0ljRS4fRECKO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4H3bmeX5EPeu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmRaGw09EaE_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ApEXcQphEq86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "MAX_LENGTH = 30\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q9XLmzsYE7wQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b32e7b70-0a0f-433a-d7e3-c4a6c4a6d3c5"
      },
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('tag', 'que', True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 1264216 sentence pairs\n",
            "Trimmed to 1263976 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "que 137206\n",
            "tag 23777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vKKJ797aLvDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19d2fbae-0158-457a-f155-9ffda7c4cbf3"
      },
      "cell_type": "code",
      "source": [
        "print(random.choice(pairs))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['salt command takes about seconds to finish', 'salt stack']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oUHOmbQtNFTQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "my99UKhqNRnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Encoder\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
      ]
    },
    {
      "metadata": {
        "id": "2qJTWGhsNHB9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_wFcq-bNtjN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Decoder"
      ]
    },
    {
      "metadata": {
        "id": "ohLato5INvm0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZn9UsL3OScF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention Decoder"
      ]
    },
    {
      "metadata": {
        "id": "FDynTIwHOHDc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NTV2U5uiOjnI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "gkT1pJehVqqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparing Training Data"
      ]
    },
    {
      "metadata": {
        "id": "526mQRRSOXgk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "99Hpba8fOvp4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "DD04BmZ3OsZ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DHdw4s56O-Db",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helpers"
      ]
    },
    {
      "metadata": {
        "id": "x-LEGQ7QPAG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSDg1OKpPAhL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('since: %s (iter: %d complete: %d%%) loss_avg: %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iyk1n46mPNlr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting Results"
      ]
    },
    {
      "metadata": {
        "id": "0J7mSzXIPGRq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wy-99ULdPZCb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "8fYQJiqCPRuj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpFhia9lPeJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('Question - ', pair[0])\n",
        "        print('Actual Tags - ', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('My Tags - ', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6E0NSiTpPyvJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and Evalutation"
      ]
    },
    {
      "metadata": {
        "id": "1V8iqKwEb9DA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The lower the **loss**, the better a model (unless the model has over-fitted to the training data). The loss is calculated on **training** and **validation** and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets."
      ]
    },
    {
      "metadata": {
        "id": "K9tTO-o9PjFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1292
        },
        "outputId": "3730d2fd-ffdc-4787-8f45-279b31ff2448"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=1000)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "since: 1m 25s (- 105m 9s) (iter: 1000 complete: 1%) loss_avg: 5.1082\n",
            "since: 2m 39s (- 97m 9s) (iter: 2000 complete: 2%) loss_avg: 4.6264\n",
            "since: 3m 55s (- 94m 8s) (iter: 3000 complete: 4%) loss_avg: 4.5478\n",
            "since: 5m 11s (- 92m 9s) (iter: 4000 complete: 5%) loss_avg: 4.5468\n",
            "since: 6m 26s (- 90m 17s) (iter: 5000 complete: 6%) loss_avg: 4.3658\n",
            "since: 7m 41s (- 88m 23s) (iter: 6000 complete: 8%) loss_avg: 4.2476\n",
            "since: 8m 57s (- 86m 57s) (iter: 7000 complete: 9%) loss_avg: 4.3120\n",
            "since: 10m 13s (- 85m 37s) (iter: 8000 complete: 10%) loss_avg: 4.1509\n",
            "since: 11m 30s (- 84m 20s) (iter: 9000 complete: 12%) loss_avg: 4.2189\n",
            "since: 12m 45s (- 82m 58s) (iter: 10000 complete: 13%) loss_avg: 4.2408\n",
            "since: 14m 1s (- 81m 38s) (iter: 11000 complete: 14%) loss_avg: 4.1484\n",
            "since: 15m 17s (- 80m 15s) (iter: 12000 complete: 16%) loss_avg: 4.0990\n",
            "since: 16m 32s (- 78m 54s) (iter: 13000 complete: 17%) loss_avg: 4.0996\n",
            "since: 17m 47s (- 77m 31s) (iter: 14000 complete: 18%) loss_avg: 4.0173\n",
            "since: 19m 3s (- 76m 13s) (iter: 15000 complete: 20%) loss_avg: 3.9973\n",
            "since: 20m 20s (- 75m 1s) (iter: 16000 complete: 21%) loss_avg: 3.9365\n",
            "since: 21m 38s (- 73m 49s) (iter: 17000 complete: 22%) loss_avg: 3.9794\n",
            "since: 22m 55s (- 72m 34s) (iter: 18000 complete: 24%) loss_avg: 3.9315\n",
            "since: 24m 10s (- 71m 15s) (iter: 19000 complete: 25%) loss_avg: 4.0413\n",
            "since: 25m 26s (- 69m 58s) (iter: 20000 complete: 26%) loss_avg: 3.8657\n",
            "since: 26m 43s (- 68m 43s) (iter: 21000 complete: 28%) loss_avg: 3.9592\n",
            "since: 27m 59s (- 67m 25s) (iter: 22000 complete: 29%) loss_avg: 3.7460\n",
            "since: 29m 16s (- 66m 10s) (iter: 23000 complete: 30%) loss_avg: 3.9467\n",
            "since: 30m 31s (- 64m 51s) (iter: 24000 complete: 32%) loss_avg: 3.7908\n",
            "since: 31m 46s (- 63m 33s) (iter: 25000 complete: 33%) loss_avg: 3.7935\n",
            "since: 33m 1s (- 62m 15s) (iter: 26000 complete: 34%) loss_avg: 3.7382\n",
            "since: 34m 18s (- 60m 59s) (iter: 27000 complete: 36%) loss_avg: 3.8583\n",
            "since: 35m 33s (- 59m 41s) (iter: 28000 complete: 37%) loss_avg: 3.7487\n",
            "since: 36m 49s (- 58m 25s) (iter: 29000 complete: 38%) loss_avg: 3.7673\n",
            "since: 38m 5s (- 57m 7s) (iter: 30000 complete: 40%) loss_avg: 3.7223\n",
            "since: 39m 21s (- 55m 51s) (iter: 31000 complete: 41%) loss_avg: 3.7616\n",
            "since: 40m 36s (- 54m 34s) (iter: 32000 complete: 42%) loss_avg: 3.6559\n",
            "since: 41m 52s (- 53m 17s) (iter: 33000 complete: 44%) loss_avg: 3.6550\n",
            "since: 43m 8s (- 52m 1s) (iter: 34000 complete: 45%) loss_avg: 3.6742\n",
            "since: 44m 25s (- 50m 45s) (iter: 35000 complete: 46%) loss_avg: 3.6857\n",
            "since: 45m 42s (- 49m 30s) (iter: 36000 complete: 48%) loss_avg: 3.6251\n",
            "since: 46m 59s (- 48m 15s) (iter: 37000 complete: 49%) loss_avg: 3.5716\n",
            "since: 48m 16s (- 47m 0s) (iter: 38000 complete: 50%) loss_avg: 3.6830\n",
            "since: 49m 33s (- 45m 44s) (iter: 39000 complete: 52%) loss_avg: 3.4864\n",
            "since: 50m 48s (- 44m 27s) (iter: 40000 complete: 53%) loss_avg: 3.6078\n",
            "since: 52m 6s (- 43m 12s) (iter: 41000 complete: 54%) loss_avg: 3.5914\n",
            "since: 53m 22s (- 41m 56s) (iter: 42000 complete: 56%) loss_avg: 3.6592\n",
            "since: 54m 40s (- 40m 41s) (iter: 43000 complete: 57%) loss_avg: 3.5087\n",
            "since: 55m 58s (- 39m 26s) (iter: 44000 complete: 58%) loss_avg: 3.5345\n",
            "since: 57m 14s (- 38m 9s) (iter: 45000 complete: 60%) loss_avg: 3.6175\n",
            "since: 58m 29s (- 36m 52s) (iter: 46000 complete: 61%) loss_avg: 3.4011\n",
            "since: 59m 45s (- 35m 35s) (iter: 47000 complete: 62%) loss_avg: 3.4991\n",
            "since: 61m 1s (- 34m 19s) (iter: 48000 complete: 64%) loss_avg: 3.6972\n",
            "since: 62m 17s (- 33m 3s) (iter: 49000 complete: 65%) loss_avg: 3.4957\n",
            "since: 63m 33s (- 31m 46s) (iter: 50000 complete: 66%) loss_avg: 3.5361\n",
            "since: 64m 49s (- 30m 30s) (iter: 51000 complete: 68%) loss_avg: 3.5515\n",
            "since: 66m 5s (- 29m 13s) (iter: 52000 complete: 69%) loss_avg: 3.4935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "since: 67m 21s (- 27m 57s) (iter: 53000 complete: 70%) loss_avg: 3.4593\n",
            "since: 68m 37s (- 26m 41s) (iter: 54000 complete: 72%) loss_avg: 3.3911\n",
            "since: 69m 55s (- 25m 25s) (iter: 55000 complete: 73%) loss_avg: 3.5353\n",
            "since: 71m 11s (- 24m 9s) (iter: 56000 complete: 74%) loss_avg: 3.4958\n",
            "since: 72m 27s (- 22m 52s) (iter: 57000 complete: 76%) loss_avg: 3.4179\n",
            "since: 73m 44s (- 21m 36s) (iter: 58000 complete: 77%) loss_avg: 3.3622\n",
            "since: 75m 0s (- 20m 20s) (iter: 59000 complete: 78%) loss_avg: 3.4983\n",
            "since: 76m 16s (- 19m 4s) (iter: 60000 complete: 80%) loss_avg: 3.4534\n",
            "since: 77m 32s (- 17m 47s) (iter: 61000 complete: 81%) loss_avg: 3.3534\n",
            "since: 78m 48s (- 16m 31s) (iter: 62000 complete: 82%) loss_avg: 3.3892\n",
            "since: 80m 5s (- 15m 15s) (iter: 63000 complete: 84%) loss_avg: 3.3586\n",
            "since: 81m 22s (- 13m 59s) (iter: 64000 complete: 85%) loss_avg: 3.2600\n",
            "since: 82m 39s (- 12m 43s) (iter: 65000 complete: 86%) loss_avg: 3.4365\n",
            "since: 83m 56s (- 11m 26s) (iter: 66000 complete: 88%) loss_avg: 3.3520\n",
            "since: 85m 12s (- 10m 10s) (iter: 67000 complete: 89%) loss_avg: 3.3406\n",
            "since: 86m 30s (- 8m 54s) (iter: 68000 complete: 90%) loss_avg: 3.3138\n",
            "since: 87m 48s (- 7m 38s) (iter: 69000 complete: 92%) loss_avg: 3.3912\n",
            "since: 89m 4s (- 6m 21s) (iter: 70000 complete: 93%) loss_avg: 3.3802\n",
            "since: 90m 20s (- 5m 5s) (iter: 71000 complete: 94%) loss_avg: 3.3527\n",
            "since: 91m 37s (- 3m 49s) (iter: 72000 complete: 96%) loss_avg: 3.2933\n",
            "since: 92m 54s (- 2m 32s) (iter: 73000 complete: 97%) loss_avg: 3.3148\n",
            "since: 94m 11s (- 1m 16s) (iter: 74000 complete: 98%) loss_avg: 3.4897\n",
            "since: 95m 28s (- 0m 0s) (iter: 75000 complete: 100%) loss_avg: 3.4168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x-vQ2M-eSVMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "2c88d328-ae43-46ae-9e77-386700c741df"
      },
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question -  efficient database structure for deep tree data\n",
            "Actual Tags -  sql database design tree hashmap bloom filter\n",
            "My Tags -  database database database <EOS>\n",
            "\n",
            "Question -  postgresql . permission to deny functions body\n",
            "Actual Tags -  sql postgresql\n",
            "My Tags -  postgresql <EOS>\n",
            "\n",
            "Question -  array and x matrices ascending sorting count repeated numbers\n",
            "Actual Tags -  visual c \n",
            "My Tags -  javascript arrays sorting sorting <EOS>\n",
            "\n",
            "Question -  java regular expression characters and matches the pattern p alpha . \n",
            "Actual Tags -  java regex pattern matching match\n",
            "My Tags -  java regex <EOS>\n",
            "\n",
            "Question -  retrying httpclient unsuccessful requests\n",
            "Actual Tags -  c dotnet httpclient httpcontent\n",
            "My Tags -  java <EOS>\n",
            "\n",
            "Question -  how to run appium script in multiple android device emulators ?\n",
            "Actual Tags -  android python python appium\n",
            "My Tags -  android <EOS>\n",
            "\n",
            "Question -  why does tomcat redirect away from angularjs petclinic ?\n",
            "Actual Tags -  java angularjs spring spring mvc tomcat\n",
            "My Tags -  angularjs angularjs angularjs <EOS>\n",
            "\n",
            "Question -  mozrequestfullscreen not working with multiple layers of canvas ?\n",
            "Actual Tags -  javascript html css\n",
            "My Tags -  javascript html canvas <EOS>\n",
            "\n",
            "Question -  whys is my mysql query returning rows but no values ?\n",
            "Actual Tags -  mysql xml file io\n",
            "My Tags -  mysql sql <EOS>\n",
            "\n",
            "Question -  advantage database server order by behaviour\n",
            "Actual Tags -  select sql order by advantage database server\n",
            "My Tags -  sql sql server server <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZE4Xdybot1zY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(encoder1.state_dict(), 'encoder.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FyPCJr6lXqw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G42TqzyflnVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "19f8774e-c33d-4e6b-b608-ddeafb96249f"
      },
      "cell_type": "code",
      "source": [
        "! ls -l"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 562936\r\n",
            "-rw-r--r-- 1 root root  50958029 May 16 11:04 attn_decoder1.pt\r\n",
            "drwxr-xr-x 1 root root      4096 Apr 30 16:29 datalab\r\n",
            "-rw-r--r-- 1 root root 142078866 May 16 10:53 encoder1\r\n",
            "-rw-r--r-- 1 root root 142078866 May 16 10:58 encoder1.pt\r\n",
            "-rw-r--r-- 1 root root 142078866 May 16 10:58 encoder.pt\r\n",
            "-rw-r--r-- 1 root root  99233319 May 16 08:55 tag-que.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-rDPhdhPl34U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('attn_decoder1.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0895md5nWR2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(attn_decoder1.state_dict(), 'attn_decoder1.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JSt-Wg5Onw6E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}